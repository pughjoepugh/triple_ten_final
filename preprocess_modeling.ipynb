{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad33e81f",
   "metadata": {},
   "source": [
    "Pre processing and modeling\n",
    "Our data is clean, now we must prepare it for modeling by spliting the data into training, validation, and testing sets. Then scale the necessary columns for modeling. The modeling stage will explore multiple models, comparing their ability to understand the data. We will also continue eda by exploring how the model made their classifications. The most appropriate model will then be tuned in hopes to improve performance, and then ultimately judged on its results from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3de60d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scikitplot as skplt\n",
    "import shap\n",
    "\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, auc, confusion_matrix, f1_score, precision_recall_curve, precision_score, recall_score, roc_auc_score, roc_curve\n",
    "from sklearn.calibration import CalibrationDisplay, calibration_curve\n",
    "\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6896639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final/ current state of data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3850a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# breakdown the data into 3 sections: 70% train, 15% validation, 15% test\n",
    "\n",
    "# create splits for features (X_) and target (y_)\n",
    "X = df.drop('Churn', axis=1)\n",
    "y = df['Churn']\n",
    "\n",
    "# Initial split, creates 70% training sets, then a bulk 30% valid and test\n",
    "X_train, X_BULK, y_train, y_BULK = train_test_split(X, y, test_size=0.3, random_state=100, stratify=y)\n",
    "\n",
    "# break the bulk apart and create valid and test splits\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_BULK, y_BULK, test_size=0.5, random_state=100, stratify=y_BULK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cc87aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the continuous numerical columns: 'MonthlyCharges', 'MemberLengthDays'\n",
    "num_col = ['MonthlyCharges', 'MemberLengthDays']\n",
    "\n",
    "# load scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# fit on training, transform everything\n",
    "X_train_scaled = scaler.fit_transform(X_train[num_col])\n",
    "X_valid_scaled = scaler.transform(X_valid[num_col])\n",
    "X_test_scaled = scaler.transform(X_test[num_col])\n",
    "\n",
    "# make a dataframe out of the transformed columns\n",
    "X_train_df = pd.DataFrame(X_train_scaled, columns=num_col, index=X_train.index)\n",
    "X_valid_df = pd.DataFrame(X_valid_scaled, columns=num_col, index=X_valid.index)\n",
    "X_test_df= pd.DataFrame(X_test_scaled, columns=num_col, index=X_test.index)\n",
    "\n",
    "# combine transformed columns dataframe with the rest of the dataset\n",
    "X_train_ids = pd.concat([X_train_df, X_train.drop(columns=num_col)], axis=1)\n",
    "X_valid_ids = pd.concat([X_valid_df, X_valid.drop(columns=num_col)], axis=1)\n",
    "X_test_ids = pd.concat([X_test_df, X_test.drop(columns=num_col)], axis=1)\n",
    "\n",
    "# remove the unique identifier column\n",
    "X_test_final = X_test_ids.drop(columns='CustomerID')\n",
    "X_valid_final = X_valid_ids.drop(columns='CustomerID')\n",
    "X_train_final = X_train_ids.drop(columns='CustomerID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b8cd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build function to train models storing results in a table/chart\n",
    "\n",
    "def models(model, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    loads the model, fits and predicts on the data provided\n",
    "    \n",
    "    args: \n",
    "    model is a list of tuples. each tuple contains the name of the model paired with the load command\n",
    "    X_train and X_test pandas dataframe containing previous split\n",
    "    y_train and y_test are series objects also created from the train test split\n",
    "    \n",
    "    returns: a print out of the performance metrics for the model\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for name, mod in model:\n",
    "        start_time = time.time()\n",
    "\n",
    "        # train\n",
    "        print(f\"Training {name}\")\n",
    "        mod.fit(X_train, y_train)\n",
    "\n",
    "        # predict\n",
    "        y_pred = mod.predict(X_test)\n",
    "        y_prob = mod.predict_proba(X_test)[:,1]\n",
    "\n",
    "        # calculate metrics\n",
    "        metrics = {\n",
    "            'Model': name,\n",
    "            'Accuracy': accuracy_score(y_test, y_pred),\n",
    "            'Precision': precision_score(y_test, y_pred),\n",
    "            'Recall': recall_score(y_test, y_pred),\n",
    "            'F1-Score': f1_score(y_test, y_pred),\n",
    "            'ROC AUC': roc_auc_score(y_test, y_prob),\n",
    "            'Training Time (s)': time.time() - start_time\n",
    "        }\n",
    "        results.append(metrics)\n",
    "    print(\"Model evaluation complete.\")\n",
    "    return pd.DataFrame(results).round(4).sort_values(by='ROC AUC', ascending=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfe23fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of models to test out\n",
    "models_list = [('Logistic Regression', LogisticRegression(solver='liblinear', \n",
    "                                                          random_state=36)),\n",
    "               ('Random Forest Classifier', RandomForestClassifier(n_estimators=100, \n",
    "                                                                   class_weight='balanced', \n",
    "                                                                   random_state=36)),\n",
    "                ('XGBoost', XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=36))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c4a49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "models(models_list, X_train_final, X_valid_final, y_train, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca6e98f",
   "metadata": {},
   "source": [
    "\n",
    "Training Logistic Regression\n",
    "Training Random Forest Classifier\n",
    "Training XGBoost\n",
    "Model evaluation complete.\n",
    "Model\tAccuracy\tPrecision\tRecall\tF1-Score\tROC AUC\tTraining Time (s)\n",
    "2\tXGBoost\t0.8570\t0.7611\t0.6714\t0.7135\t0.8975\t1.1761\n",
    "0\tLogistic Regression\t0.7917\t0.6250\t0.5357\t0.5769\t0.8357\t0.0189\n",
    "1\tRandom Forest Classifier\t0.7983\t0.6558\t0.5036\t0.5697\t0.8302\t0.4749\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902f61bb",
   "metadata": {},
   "source": [
    "thoughts\n",
    "___\n",
    "All three models performed well. Either the problem was simple to solve or our cleaning and processing of data was done well. Random Forest had the worst results, we will stop exploring it now. Logistic regression did well and for models it is more transparent as to why predictions are made. XGBoost did great already. an Auc-roc score of near 0.89 out of the box is great."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8855a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore logistic regression prediction coeficients\n",
    "# same model from before\n",
    "log_reg = LogisticRegression(solver='liblinear', random_state=36)\n",
    "log_reg.fit(X_train_final, y_train)\n",
    "\n",
    "# store coefficients and feature names\n",
    "coefficients = log_reg.coef_[0]\n",
    "feature_names = X_train_final.columns\n",
    "\n",
    "# turn into a dataframe for viewing\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': coefficients,\n",
    "    'Absolute_Coefficient': np.abs(coefficients)})\n",
    "\n",
    "# add odds_ratio\n",
    "coef_df['Odds_Ratio'] = np.exp(coef_df['Coefficient'])\n",
    "\n",
    "# display the dataframe\n",
    "print('Logistic regression coefficients and odds ratios')\n",
    "print('-'*80)\n",
    "coef_df.sort_values(by='Absolute_Coefficient', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0834178",
   "metadata": {},
   "source": [
    "thoughts\n",
    "___\n",
    "Previously we noticed that those who have churned had higher average monthly bills. Based on the Logistic regression coefficients, monthly charges had the highest positive association to churning. Monthly charges was one of the standard scaled features, with an odds_ratio of 2.009 that means that for each unit increase (in this case the unit is standard deviation) the odds of churning increases by 100% or simply, it doubles.\n",
    "\n",
    "Our other high positive association features were one hot encoded. So the odds ratio of 1.416 for month to month contracts simply means theres a 41.6% greater odds of churning compared to others plans.\n",
    "\n",
    "Paperless Billing actually contributes to churn, this might tell you more about the type of customer who accepts this feature. Most of my personal bills offer a discount for paperless billing. Smaller monthly bills retain customers, so these two features appear to be counter-intuitive \n",
    "\n",
    "The two greatest predictors or member retension are two year contracts and member length in days. This makes sense, being locked into a two year contract prevents churn for some time, and during that time member length in days goes up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9738c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original data, unscaled essentially\n",
    "df['MonthlyCharges'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4d939b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "thoughts\n",
    "____\n",
    "A monthly bill increase of \\\\$30.09 will double the odds of being churned. That is actually quite a large increase. The average bill is \\\\$64.74,  I do not know specifically what could cost \\\\$30 a month. \n",
    "\n",
    "We still havent tuned models, in order to more accurately identify churn candidates, however we have uncovered that bigger bills are leading to churn. It is important for InterConnect to investigate which services, add ons or bundling is leading to high bill averages. Potentially polling customers (data collection) on the preceived value of their contract. Are customers with higher bills being upsold into these contracts that they ultimately can't wait to get out of?  \n",
    "\n",
    "We already know InterConnect has plans to be proactive in offering discounts or some sort of promotion to clients who are at risk of churn. That is this whole project. We also have identified that Internet Security and Tech support contribute to customer retension, offering a decrease in churn odds of 43% and 41% respectfully. They are high value features. We do not have information about the opperating costs associated with these features, consider offering these features as promotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba929a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning XGBoost\n",
    "# record run time\n",
    "start_time = time.time()\n",
    "\n",
    "# initialize the model\n",
    "boost_model = XGBClassifier(use_label_encoder=False, \n",
    "                            eval_metric='logloss', \n",
    "                            random_state=36,\n",
    "                           )\n",
    "# create a parameter dictionary for random search cv, using ranges for each one to \n",
    "params = {\n",
    "    'n_estimators': randint(100, 400),\n",
    "    'learning_rate': uniform(0.1, 0.08),\n",
    "    'max_depth': randint(4, 7),\n",
    "    'subsample': uniform(0.8, 0.2),            \n",
    "    'colsample_bytree': uniform(0.7, 0.3),     \n",
    "    'gamma': uniform(0.1, 0.2),                 \n",
    "    'lambda': uniform(0.8, 0.4),   \n",
    "    'alpha': uniform(0, 0.2)     \n",
    "}\n",
    "\n",
    "# set up random search settings, grading by our desired metric (auc-roc)\n",
    "random_search = RandomizedSearchCV(estimator=boost_model,\n",
    "                                  param_distributions=params,\n",
    "                                  n_iter=100,\n",
    "                                  scoring='roc_auc',\n",
    "                                  cv=5,\n",
    "                                  verbose=1,\n",
    "                                  random_state=36,\n",
    "                                  n_jobs=1)\n",
    "\n",
    "\n",
    "# fit to find the best params\n",
    "random_search.fit(X_train_final, y_train)\n",
    "\n",
    "# record stop run time\n",
    "end_time = time.time()\n",
    "\n",
    "# display best parameters abnd best results\n",
    "print(f'Run time: {end_time-start_time}\\n with the best params of:')\n",
    "print(random_search.best_params_)\n",
    "print()\n",
    "print(f'Best result: {random_search.best_score_:.2f}')\n",
    "\n",
    "# store the best model settings\n",
    "best_xgb = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6a2dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the best model settings to make predictions on the validation set\n",
    "\n",
    "# record run time\n",
    "start_eval_time = time.time()\n",
    "\n",
    "# store churn predictions (0,1)\n",
    "y_pred_tuned_xgb_valid = best_xgb.predict(X_valid_final)\n",
    "\n",
    "# store the churn probability (0.0 - 1.0), these are the models calculated probability of churn, default threshold is 0.5, above = churn, below no-churn\n",
    "y_prob_tuned_xgb_valid = best_xgb.predict_proba(X_valid_final)[:, 1]\n",
    "\n",
    "# results/ metrics\n",
    "tuned_xgb_valid_metrics = {\n",
    "    'Accuracy': accuracy_score(y_valid, y_pred_tuned_xgb_valid),\n",
    "    'Precision': precision_score(y_valid, y_pred_tuned_xgb_valid),\n",
    "    'Recall': recall_score(y_valid, y_pred_tuned_xgb_valid),\n",
    "    'F1-Score': f1_score(y_valid, y_pred_tuned_xgb_valid),\n",
    "    'ROC AUC': roc_auc_score(y_valid, y_prob_tuned_xgb_valid)\n",
    "}\n",
    "\n",
    "print(\"Tuned XGBoost Model Metrics (on Validation Set):\")\n",
    "for metric, value in tuned_xgb_valid_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# record end run time\n",
    "print(f\"\\nEvaluation completed in {(time.time() - start_eval_time):.4f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b988babc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the random search parametes tested, just in case\n",
    "random_results_df = pd.DataFrame(random_search.cv_results_)\n",
    "\n",
    "random_results_df.sort_values(by='rank_test_score', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331025e2",
   "metadata": {},
   "source": [
    "thoughts\n",
    "____\n",
    "We now have a tuned model that is continueing to perform well on unseen data. The AUC-ROC score is 0.9, the recall is only 0.68 though. Recall is important for this problem because the churn category was the positive category. Recall is correctly predicted churns ( True positives ) divided by the total of correctly predicted churns ( true positves again ) + churners that the model missed ( false negatives). For this business problem the downside of miss labeling a non-churn candidate as churn-able, simply means we offered an extra discount/ promotion (slight decrease in revenue) while missing someone who would churn is a greater decrease in revenue.\n",
    "\n",
    "We are able to adjust the classification threshold to purposely predict more churn. This should increase False positives (people who receive promotions etc.) and decrease False Negatives (People who churn without receiving a promotion). InterConnect would need to provide more information on how they calculate customer lifetime value or what would be remaining based on their current memberlength. Expected success rate of promotion etc. Combined they could plot the predicted cost to the company at each possible threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd34138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test our best model's performance on the unseen data\n",
    "\n",
    "# record run time\n",
    "start_eval_time = time.time()\n",
    "\n",
    "# store predictions and prediction probabilities\n",
    "y_pred_tuned_xgb_test = best_xgb.predict(X_test_final)\n",
    "y_prob_tuned_xgb_test = best_xgb.predict_proba(X_test_final)[:, 1] \n",
    "\n",
    "# results/ metrics dictionary\n",
    "tuned_xgb_test_metrics = {\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_tuned_xgb_test),\n",
    "    'Precision': precision_score(y_test, y_pred_tuned_xgb_test),\n",
    "    'Recall': recall_score(y_test, y_pred_tuned_xgb_test),\n",
    "    'F1-Score': f1_score(y_test, y_pred_tuned_xgb_test),\n",
    "    'ROC AUC': roc_auc_score(y_test, y_prob_tuned_xgb_test)\n",
    "}\n",
    "\n",
    "# display results\n",
    "print(\"Tuned XGBoost Model Metrics (on TEST Set):\")\n",
    "for metric, value in tuned_xgb_test_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "# end run time\n",
    "print(f\"\\nEvaluation completed in {(time.time() - start_eval_time):.4f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ae14b6",
   "metadata": {},
   "source": [
    "thoughts\n",
    "____\n",
    "The model performed very similiar. This is great, results on unseen data are expected to be slightly worse. Recall did decrease slightly, however we will explore that shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3094c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display prediction results with through a confusion matrix (TP, TN, FP, FN)\n",
    "\n",
    "# plot the confusion matrix with labels for easier reading\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_tuned_xgb_test),\n",
    "            annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=['Predicted No Churn', 'Predicted Churn'],\n",
    "            yticklabels=['Actual No Churn', 'Actual Churn'])\n",
    "plt.title(f'Confusion Matrix on Test Set')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b634787",
   "metadata": {},
   "source": [
    "thoughts\n",
    "___\n",
    "We knew the performance metrics we decent which means the True Positive and True Negatives were going to be the largest groups (correct predictions). However our specific business problem isn't acknowledged by the model. \n",
    "\n",
    "- This data contained 281 accounts that churned.\n",
    "- correctly identified 65% of them, so 35% wouldn't receive promotions, not great.\n",
    "- the model identified 238 accounts suitable for churn (promotions), and 22.7% of promotions went to accounts that had not churned AT THIS TIME. just because they hadn't churned doesn't mean they weren't about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618bf4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot precision recall curve, \n",
    "plt.figure(figsize=(8, 6))\n",
    "precision, recall, thresholds_pr = precision_recall_curve(y_test, y_prob_tuned_xgb_test)\n",
    "plt.plot(recall, precision, label='Precision-Recall curve')\n",
    "plt.plot([0.6548, 0.6548], [0.0, 1], color='red', lw=2, linestyle='--', label='Recall at default threshold')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall curve on test set')\n",
    "plt.legend(loc='lower left')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19c8b80",
   "metadata": {},
   "source": [
    "thoughts\n",
    "____\n",
    "From the percision recall curve, we can see that by increasing the recall metric we will see a drop in percision. This is normal. How much percision are we willing to lose?\n",
    "\n",
    "Percision (y axis) high percision means our targeting effort is efficient. No-or-fewer unnecessary promotions.\n",
    "\n",
    "Recall (x axis) high recall means everyone in danger of churning gets a promotion, and the error is extra promotions. I believe this is the better option, however if InterConnect doesn't like to appearance of being the 'coupon' company, or the discount-guys, then high percision would be important to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb113519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "fpr, tpr, thresholds_roc = roc_curve(y_test, y_prob_tuned_xgb_test)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.plot(fpr, tpr, color='orange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
    "plt.plot([0, 1], [0.6548, 0.6548], color='red', lw=2, linestyle='--', label=\"Model's Recall\")\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate (Recall)')\n",
    "plt.title('Receiver operating characteristic (ROC) curve on test set')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d992668",
   "metadata": {},
   "source": [
    "thoughts\n",
    "____\n",
    "From the ROC curve we can see that our model is performing a lot better than a random model. We already knew this. We can also see that adjusting the prediction threshold could increase recall. The ROC apex is above the current threshold position. Meaning we can increase recall faster than the increase of false positives. Capturing churners faster than offering extra discounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e540151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with all the test information, results, probabilities\n",
    "combined_test = X_test_final.copy()\n",
    "combined_test['Actual'] = y_test\n",
    "combined_test['Prediction'] = y_pred_tuned_xgb_test\n",
    "combined_test['Probability'] = y_prob_tuned_xgb_test\n",
    "\n",
    "# create subsets for FN FP TN TP\n",
    "tp = combined_test[(combined_test['Actual'] == 1) & (combined_test['Prediction'] == 1)]\n",
    "tn = combined_test[(combined_test['Actual'] == 0) & (combined_test['Prediction'] == 0)]\n",
    "fp = combined_test[(combined_test['Actual'] == 0) & (combined_test['Prediction'] == 1)]\n",
    "fn = combined_test[(combined_test['Actual'] == 1) & (combined_test['Prediction'] == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a262c0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# probability histograms for predictions \n",
    "\n",
    "plt.figure(figsize=(15, 30))\n",
    "bins = np.linspace(0, 1, 50)\n",
    "\n",
    "# plot 1: True Positives and False Negatives (Actual Churners)\n",
    "plt.subplot(4, 2, 1)\n",
    "sns.histplot(tp['Probability'], bins=bins, color='green', label='True Positives', kde=True, stat='density', alpha=0.5)\n",
    "sns.histplot(fn['Probability'], bins=bins, color='red', label='False Negatives', kde=True, stat='density', alpha=0.5)\n",
    "plt.axvline(x=0.5, color='black', linestyle='--', label=f'Threshold ({0.5})')\n",
    "plt.title('Actual Churners: TP vs FN Probability Distribution')\n",
    "plt.xlabel('Predicted Probability of Churn')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--')\n",
    "\n",
    "# plot 2: True Negatives and False Positives (Actual Non-Churners)\n",
    "plt.subplot(4, 2, 2)\n",
    "sns.histplot(tn['Probability'], bins=bins, color='blue', label='True Negatives', kde=True, stat='density', alpha=0.5)\n",
    "sns.histplot(fp['Probability'], bins=bins, color='orange', label='False Positives', kde=True, stat='density', alpha=0.5)\n",
    "plt.axvline(x=0.5, color='black', linestyle='--', label=f'Threshold ({0.5})')\n",
    "plt.title('Actual Non-Churners: TN vs FP Probability Distribution')\n",
    "plt.xlabel('Predicted Probability of Churn')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--')\n",
    "\n",
    "# plot 3: All Correct Predictions (TP + TN)\n",
    "plt.subplot(4, 2, 3)\n",
    "sns.histplot(tp['Probability'], bins=bins, color='green', label='True Positives', kde=True, stat='density', alpha=0.5)\n",
    "sns.histplot(tn['Probability'], bins=bins, color='blue', label='True Negatives', kde=True, stat='density', alpha=0.5)\n",
    "plt.axvline(x=0.5, color='black', linestyle='--', label=f'Threshold ({0.5})')\n",
    "plt.title('Correct Predictions: TP & TN Probability Distribution')\n",
    "plt.xlabel('Predicted Probability of Churn')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--')\n",
    "\n",
    "# plot 4: All Incorrect Predictions (FP + FN)\n",
    "plt.subplot(4, 2, 4)\n",
    "sns.histplot(fp['Probability'], bins=bins, color='orange', label='False Positives', kde=True, stat='density', alpha=0.5)\n",
    "sns.histplot(fn['Probability'], bins=bins, color='red', label='False Negatives', kde=True, stat='density', alpha=0.5)\n",
    "plt.axvline(x=0.5, color='black', linestyle='--', label=f'Threshold ({0.5})')\n",
    "plt.title('Incorrect Predictions: FP & FN Probability Distribution')\n",
    "plt.xlabel('Predicted Probability of Churn')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--')\n",
    "\n",
    "# plot 5: All Negative Predictions (TN + FN)\n",
    "plt.subplot(4, 2, 5)\n",
    "sns.histplot(tn['Probability'], bins=bins, color='blue', label='True Negatives', kde=True, stat='density', alpha=0.5)\n",
    "sns.histplot(fn['Probability'], bins=bins, color='red', label='False Negatives', kde=True, stat='density', alpha=0.5)\n",
    "plt.axvline(x=0.5, color='black', linestyle='--', label=f'Threshold ({0.5})')\n",
    "plt.title('Negative Predictions: TN & FN Probability Distribution')\n",
    "plt.xlabel('Predicted Probability of Churn')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--')\n",
    "\n",
    "\n",
    "# plot 6: All Positive Predictions (TP + FP)\n",
    "plt.subplot(4, 2, 6)\n",
    "sns.histplot(tp['Probability'], bins=bins, color='green', label='True Positives', kde=True, stat='density', alpha=0.5)\n",
    "sns.histplot(fp['Probability'], bins=bins, color='orange', label='False Positives', kde=True, stat='density', alpha=0.5)\n",
    "plt.axvline(x=0.5, color='black', linestyle='--', label=f'Threshold ({0.5})')\n",
    "plt.title('Positive Predictions: TP & FP Probability Distribution')\n",
    "plt.xlabel('Predicted Probability of Churn')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--')\n",
    "\n",
    "# plot 7: All Predictions\n",
    "plt.subplot(4, 2, 7)\n",
    "sns.histplot(tp['Probability'], bins=bins, color='green', label='True Positives', kde=True, stat='density', alpha=0.5)\n",
    "sns.histplot(fp['Probability'], bins=bins, color='orange', label='False Positives', kde=True, stat='density', alpha=0.5)\n",
    "sns.histplot(tn['Probability'], bins=bins, color='blue', label='True Negatives', kde=True, stat='density', alpha=0.5)\n",
    "sns.histplot(fn['Probability'], bins=bins, color='red', label='False Negatives', kde=True, stat='density', alpha=0.5)\n",
    "plt.axvline(x=0.5, color='black', linestyle='--', label=f'Threshold ({0.5})')\n",
    "plt.title('All Prediction Probabilities')\n",
    "plt.xlabel('Predicted Probability of Churn')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--')\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a68843",
   "metadata": {},
   "source": [
    "thoughts\n",
    "___\n",
    "- Correct Predictions: probabilities were most decisive, the spread grows away from the threshold\n",
    "- Incorrect Predictions: noisy histogram. False positives (customers predicted to churn, that have not yet) has higher density around the threshold.\n",
    "- False Negatives: those who churned that the model missed, these are the most important for increasing profit. These are customers who would not receive a promotion.\n",
    "- Even without knowing how InterConnect calculates lifetime value for customers, we can see (plot 5: all negative predictions) if our threshold became 0.3 for example we would capture a lot more of the False Negatives than providing extra discounts to non-churners.\n",
    "- Going more extreme with threshold adjustments, the kde lines cross at about 0.125.\n",
    "- Because essentially all the noise in the incorrect predictions, theres potential to do some more feature engineering. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b044bef",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "____\n",
    "We set out to develop a predictive model for customer churn to help InterConnect retain at risk customers. I've successfully constructed and extensively tuned an XGBoost model to predict churn for InterConnect. \n",
    "\n",
    "The models performance on test set (probability threshold 0.5): \n",
    "Accuracy: 0.8571, the model correctly identified 85.7% of client's status\n",
    "Precision: 0.7731, 77.3% of churn predictions were correct\n",
    "Recall: 0.6548, 65.5% of actual churners were identified \n",
    "F1-Score: 0.7091, a balance of precision and recall\n",
    "ROC AUC: 0.8988, models ability to distinguish between the two classes, 1 is perfect\n",
    "\n",
    "The model was able to confidently predict TP TN. The histogram of probabilities from these classes were highly skewed. There was some noise when it came to the incorrect predictions. Noise could be removed with further feature engineering. Even with the noise, a different threshold can be chosen by InterConnect based on how they calculate Customer Lifetime Value. We also do not know the specifics of the proposed promotion. Different promotions would lead to a different threshold. I suggest set the threshold to a number that provides the smallest decrease in revenue. Adjusting the threshold down, would increase recall at the expense of precision. The model would begin to err on the side of predicting churn. This would increase the number of promotions sent out.\n",
    "\n",
    "The model is technically performing well, but it is only as useful as it's ability to retain customers. We don't have information on who used tech support, if those calls/ session were positive or not. Has a customer missed a bill? I am sure InterConnect is aware humans are human, so they are only so predictable. Does this model fit the scale of marketing campaign InterConnect had in mind? i.e. were they thinking of offering hundreds of discounts? thousands? We can continue to work together on this. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
